{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cpt0988/Ml-class/blob/main/Project/Image_Classification_project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset\n",
        "The following is from Alex Frizhevsky's description of the dataset.\n",
        "\n",
        "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "Download\n",
        "Discussion\n",
        "This is similar to the MNIST and clothing datasets. In those the images were 28x28x1. In this one the images are 32x32x3. Just like MNIST and clothing, this dataset has 10 possible labels.\n",
        "\n",
        "I am hoping that you see that this is a slight remix of our work with MNIST and clothing.\n",
        "\n",
        "This dataset is widely used today for performance testing of new deep learning methods."
      ],
      "metadata": {
        "id": "wH9r22jYn92q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the importing of various libraries and functions.\n",
        "\n",
        "* From keras.datasets import cifar10: imports the CIFAR-10 dataset from the Keras library.\n",
        "* From keras import models: imports the models module from Keras, which provides a way to define, compile, and train neural network models.\n",
        "* From keras import layers: imports the layers module from Keras, which provides various types of layers that can be added to a neural network model, such as convolutional, pooling, and fully connected layers.\n",
        "* From keras.utils import to_categorical: imports the to_categorical function from Keras utilities, which is used to convert class labels (integers) to one-hot encoded vectors. \n",
        "* import numpy as np: imports the NumPy library,\n",
        "* from keras.layers import Dense: imports the Dense class from the Keras layers module, which represents a fully connected (dense) layer in a neural network.\n",
        "\n",
        "* from keras import optimizers: imports the optimizers module from Keras, which provides various optimization algorithms, such as stochastic gradient descent (SGD), Adam, and RMSprop, that can be used for training a neural network model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pb69MdqboBPU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VqWNxdJP29Ab"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "#from keras.engine.functional import network_serialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, loads the CIFAR-10 dataset from the Keras library. The first set contains the training data (x_train, y_train) and the second one contains the testing data (x_test, y_test).I use 'assert' to check if the actual shapes of the loaded data match the expected shapes.The shape for x_train is (50000, 32, 32, 3), which means there are 50,000 images, each with a size of 32x32 pixels and 3 color channels (red, green, and blue). Similarly, the expected shape for x_test is (10000, 32, 32, 3), which means there are 10,000 images with the same dimensions.\n",
        "Also for the test set, the shape for y_train is (50000, 1), meaning there are 50,000 labels, one for each training image. Similarly, the expected shape for y_test is (10000, 1), meaning there are 10,000 labels, one for each testing image"
      ],
      "metadata": {
        "id": "tx5ZyGt1sAQ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mmmlB0sM9B21"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test)  = cifar10.load_data()\n",
        "assert x_train.shape == (50000, 32, 32, 3)\n",
        "assert x_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I normalize the pixel values the images are originally in the range of 0 to 255. By dividing the pixel values by 255, you are rescaling them to be in the range of 0 to 1. Then, the convert the labels to one-hot encoded vectors from a integer values.\n",
        "Then, I construct a sequential model using Keras, which means you can add layers one by one in a linear stack.Then added a serious series of convolutional, max-pooling, and dense layers to the model.After the convolutional and pooling layers, I flatten the output so that it can be fed into the fully connected layers.I adde a dense layer and use the ReLU activation function and the softmax activation function to produce class probabilities."
      ],
      "metadata": {
        "id": "vY4laGPQu2YE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ImFK4CEQDNyb"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I compiled the model by specifying the optimizer, loss function, and evaluation metric. The optimizer used here is 'rmsprop', which is an adaptive learning rate optimization algorithm. The loss function is 'categorical_crossentropy', which is suitable for multi-class classification problems, and the evaluation metric is 'accuracy'.I train the model using the fit function, which takes the training data (x_train, y_train) as input along with the number of epochs and batch size. The validation_data parameter is set to the testing dataset (x_test, y_test) to evaluate the model's performance on unseen data during training. \n",
        "\n",
        "After training,I evaluated the model's performance on the test dataset using the evaluate function, which returns the test loss and test accuracy. You then print the test accuracy."
      ],
      "metadata": {
        "id": "0e1jZoqexHGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Test the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XF80qFSY81O",
        "outputId": "8681c563-f6fb-4b5a-919d-e4d06f8b7372"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 16s 5ms/step - loss: 1.6372 - accuracy: 0.4053 - val_loss: 1.4274 - val_accuracy: 0.4813\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.2298 - accuracy: 0.5645 - val_loss: 1.4744 - val_accuracy: 0.4934\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.0439 - accuracy: 0.6365 - val_loss: 1.2394 - val_accuracy: 0.5779\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.9260 - accuracy: 0.6757 - val_loss: 1.0162 - val_accuracy: 0.6418\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.8355 - accuracy: 0.7093 - val_loss: 0.9959 - val_accuracy: 0.6466\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.7679 - accuracy: 0.7339 - val_loss: 0.8272 - val_accuracy: 0.7132\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7071 - accuracy: 0.7534 - val_loss: 1.0674 - val_accuracy: 0.6621\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6530 - accuracy: 0.7741 - val_loss: 0.8671 - val_accuracy: 0.7080\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6067 - accuracy: 0.7896 - val_loss: 0.9730 - val_accuracy: 0.6877\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.5637 - accuracy: 0.8030 - val_loss: 0.9512 - val_accuracy: 0.6977\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9512 - accuracy: 0.6977\n",
            "Test accuracy: 0.697700023651123\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOICm5BnlCFlRzIjcWnnKfI",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}